% Based on template by MR

\documentclass[12pt,twoside,notitlepage]{report}

\usepackage{a4}
\usepackage{verbatim}

\input{epsf}                            % to allow postscript inclusions

\raggedbottom                           % try to avoid widows and orphans
\sloppy
\clubpenalty1000%
\widowpenalty1000%

\addtolength{\oddsidemargin}{6mm}       % adjust margins
\addtolength{\evensidemargin}{-8mm}

\renewcommand{\baselinestretch}{1.1}    % adjust line spacing to make
                                        % more readable
\newcommand{\msg}[1] {{\bf #1}}
\begin{document}

\bibliographystyle{plain}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title


\pagestyle{empty}

\hfill{\LARGE \bf Charlie Shepherd}

\vspace*{60mm}
\begin{center}
\Huge
{\bf PDB: A Distributed Database Based on Paxos} \\
\vspace*{5mm}
Computer Science Tripos \\
\vspace*{5mm}
Churchill College \\
\vspace*{5mm}
\today  % today's date
\end{center}

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma, table of contents and list of figures

\setcounter{page}{1}
\pagenumbering{roman}
\pagestyle{plain}

\chapter*{Proforma}

{\large
\begin{tabular}{ll}
Name:               & \bf Charlie Shepherd                        \\
College:            & \bf Churchill College                     \\
Project Title:      & \bf PDB: A Distributed Database Based on Paxos \\
Examination:        & \bf Computer Science Tripos, July 2013        \\
Word Count:         & \bf wordcount \\
Project Originator: & Charlie Shepherd                    \\
Supervisor:         & Stephen Cross                    \\
\end{tabular}
}


\section*{Original Aims of the Project}

\subsection*{Project aims}

I aim to implement a distributed database. This will be based on the Paxos protocol.

\subsubsection*{Paxos}

The first half of the project is to implement the Paxos protocol. This will be done in a module,
providing an interface which the database can then use to 

The project must correctly implement the Paxos protocol.  The library must be capable of forming a
running network, in particular dynamic leader election, as well as achieving consensus on a
key/value store across the network.

The database must implement a subset of SQL, specifically. The database must have all ACID
properties, that is.

\section*{Work Completed}

All that has been completed appears in this dissertation.

\section*{Special Difficulties}

None.

\newpage
\section*{Declaration}

I, Charlie Shepherd of Churchill College, being a candidate for Part II of the Computer Science
Tripos, hereby declare that this dissertation and the work described in it are my own work,
unaided except as may be specified below, and that the dissertation does not contain material that
has already been used to any substantial extent for a comparable purpose.

\bigskip
\leftline{Signed: }

\medskip
\leftline{Date: \today}

\cleardoublepage

\tableofcontents

\listoffigures

\newpage
\section*{Acknowledgements}

Acknowledge various people

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% now for the chapters

\cleardoublepage        % just to make sure before the page numbering
                        % is changed

\setcounter{page}{1}
\pagenumbering{arabic}
\pagestyle{headings}

\chapter{Introduction}

\section{Overview}

\section{Motivation}

I set out to build a distributed database based on Paxos. Paxos was first described in Leslie
Lamport's paper \emph{The Part Time Parliament} \cite{lamport98}.

how it integrates with ecosystem (aka what's already out there).


talk about distributed, p2p advantages


\section{Distributed Consensus Problem}

Consensus is an integral problem in distributed systems. The consensus problem is that of getting
all nodes in a distributed system to agree on a value. Formally, an algorithm that satisfies the
consensus problem satisfies three properties:

\begin{enumerate}
\item Agreement - all nodes must decide the same value.
\item Validity - the value that is decided upon must have been proposed by some node in the
	network.
\item Termination - all nodes eventually decide on a value.
\end{enumerate}

Decision is defined as follows - a process must decide on a value only once, and cannot change the
value once it has been decided.

\subsection*{Failure Modes}

Distributed networks must be able to deal with failure in a number of forms. Nodes may fail and
stop executing completely, this is known as the \emph{fail-stop} model. In practice, a more accurate
model is the \emph{fail-recover} model. This is where a node stops executing, then resumes execution
at a later time. This more accurately models real life, where asynchronous networks mean that it
may take an arbitrarily long time to receive a message, and that messages may arrive out of order
or not at all. The most general failure model is that of \emph{Byzantine failure}, where nodes may
respond incorrectly or even deliberately lie in order to mislead the protocol. In practice this is
unlikely, and can be mediated using checksums, message digests and authentication. In the
Preparation chapter I will go into detail about the types of failure that my database can handle.

\subsection*{2 Phase Commit}

2 Phase Commit (2PC) is one of the simplest consensus protocol, and one of the most brittle. It
has two phases:

\begin{enumerate}
\item The co-ordinator (the node initiating the protocol) sends a \msg{PROPOSE} message to each
	node in a cohort of size $N$, asking them to accept the value proposed.
\item If all nodes respond with a \msg{YES} message, send a \msg{CONFIRM} message. Otherwise, if
	any node responds with a \msg{NO} message, send an \msg{ABORT} message to all nodes.
\end{enumerate}

2PC solves the consensus problem in a failure free network. However if we can have failures then
the protocol can suffer from a number of limitations. If the co-ordinator crashes before sending
any messages, we satisfy consensus trivially. However, if the co-ordinator crashes after sending
$x$ messages, where $1 \le x < N$, the protocol cannot continue - it is blocked on the
co-ordinator resuming, and if it never resumes then some members of the cohort are blocked
permanently. In fact, once a node has sent a \msg{YES} message, it is blocked until it receives a
response. This is a big disadvantage for a concurrent system. While there are extensions to
resolve the problem of a crashing co-ordinator, these do not fix the fundamental problem of using
a blocking protocol in an asynchronous network. (These extensions often involve a "watchdog" or
"recovery node". This is still not a satisfactory solution as the simultaneous crash of the
co-ordinator and a cohort member means the state of the network is not recoverable (ie, we cannot
tell if the cohort member who crashed voted \msg{YES} or \msg{NO}.))

\subsection*{3 Phase Commit}

3 Phase Commit (3PC) is a modification of 2PC that turns it from a synchronous protocol into an
asynchronous protocol. This is done by adding a third phase, so that we can use timeouts to assert
the state of the system at any point in time. The three phases are:

\begin{enumerate}
\item Prepare - the coordinator
\item Pre-commit - 
\item Finalise commit - 
\end{enumerate}

This fixes the problem of node failure, but still suffers from a significant problem. If there is
a network partition, and all nodes who voted \msg{YES} are in one half and all nodes who voted
\msg{NO} are in the other, both partitions will recover into different, inconsistent states.
The CAP theorem (XXX: cite?) says that we cannot achieve both consensus and availability
during a network partition - we must choose one. 3PC opts for availability, but for a distributed
database with ACID properties it is preferable to have consensus. Paxos guarantees consensus at
the cost of lack of availability in a network partition.

\subsection*{Paxos}

Paxos is a generalisation of 2PC and 3PC that handles more failure modes. I will go into a
detailed explanation in the Preparation chapter, here I will just summarise it. 

\section{Databases}

\subsection*{ACID}

Commonly all databases provide ACID properties. ACID stands for:

\begin{itemize}
\item Atomic - an operation is either performed or is not performed.
\item Consistent - the database remains in a consistent state at all times.
\item Isolated - one operation cannot see the intermediate state due to another operation occurring
	at the same time.
\item Durable - once an operation is "committed" it is permanently stored - the effects of it will
	not be lost.
\end{itemize}

There are many reasons why ACID is a key requirement of most databases. ACID makes it far easier
for application developers to reason about concurrency in a system where there may be multiple
clients reading and writing the same data simultaneously. It also enable effective abstraction, as
the atomicity requirement in particular gives the well defined characteristics when an operation
or transaction fails. ACID also provides clear guarantees with regard to data stability and
longevity, in particular how this relates to consistency, so that users can be clear as to the
overall state of the system.

\subsection*{Centralisation vs Distributed}

Databases are generally found in one of two topologies - centralised or distributed. These
different topologies are used in different situations and to different ends.

A centralised database is a single node in a single location.
In general, centralised databases are simpler in design and on the whole faster than distributed
databases.
Many general properties of centralised vs. distributed systems apply to centralised databases -
they are easier to perform organise, to edit, to query and to backup. May slow down under load. It
is easier to maintain the integrity of data on a centralised database, as there is only one
"current" version of the data under consideration.

In contrast, distributed databases are more resilient and scalable than centralised databases.
There is no longer a single point of failure, and they can be extended by adding nodes, although
this will have a point of diminishing returns. Distributed databases are often more complex in
design, as they have to ensure consistency between nodes (although there is a recent trend to
relax this constraint in certain "NoSQL" databases. Here I will only consider traditional RDBMS
systems). In particular, distributed databases can be slow accessing non local data.

\subsection*{Existing Paxos databases}

Paxos has recently become a very popular algorithm for ensuring distributed consensus. One good
example is Google using Paxos for a distributed lock service called "Chubby". This underpins their
BigTable distributed database which is used across Google.

(XXX: Need some more examples.)

\cleardoublepage

\chapter{Preparation}

\section{Paxos}

\subsection{Introduction}

Paxos is a distributed consensus protocol. It was developed by Leslie Lamport at Microsoft
research when he was trying to disprove its existence. Paxos is failure tolerant for up to $F$
simultaneous failures in a network of $2F + 1$ nodes.

Paxos is actually a family of protocols, based around the same main algorithm. The Paxos algorithm
is an algorithm for agreeing on a single value across a network of processors. Paxos provides
three guarantees: safety, liveness and non-triviality:

\begin{enumerate}
	\item Consistency: Only one value is chosen.
	\item Liveness: If a value is proposed, eventually some value is chosen.
	\item Non-triviality: only proposed values may be chosen.
\end{enumerate}

Paxos can tolerate certain kinds of failures. These are: messages being delivered late or not at
all, etc XXX.

However Paxos cannot tolerate "rogue" processes, that is, processes deliberately sending malicious
or incorrect messages. There is a variant of Paxos called Byzantine Paxos which can tolerate this,
albeit at a failure tolerance of $F$ for $3F + 1$ nodes. I will not go into this variant further
in this dissertation.

\subsection{Definitions}

\subsubsection*{Proposal Numbering}

One of the assumptions that Paxos makes is that every proposal has a unique proposal number. This
is necessary so that proposals have a \emph{total order}, ie we can compare any two proposals to
find the maximum ordered proposal. The conventional way to achieve this is to define a proposal
number as a 2-tuple of (sequence number, node address). These can be compared lexicographically,
and as node addresses are unique, every proposal number will be unique. In practice I plan to use
UUIDs as node identifiers, in order to be confident on uniqueness.

\subsubsection*{Quorums}

Paxos relies on quorums to ensure that even in the event of failures, consistency is preserved. A
quorum is defined as a majority of nodes. In some versions of Paxos, weighting can be used to
define quorums, however here I will use unweighted quorums.

\subsubsection*{Messages}

Paxos utilises several message types:

\begin{itemize}
\item {\bf Prepare(n)}, where $n$ is the proposal number of the message.
\item {\bf Promise(n, v)}, where $n$ is the highest accepted proposal number by the recipient and
	$v$ is the value of that proposal.
\item {\bf AcceptRequest(n, v)}, where $n$ is the proposal number of the proposal and $v$ is the
	value of that proposal.
\item {\bf AcceptNotify(n, v)}, where $n$ is the proposal number of the proposal and $v$ is the value
	of that proposal.
\end{itemize}

\subsection{How it works}

A round of Paxos consists of two stages:

\subsubsection*{Phase 1a}

To initiate a round of Paxos, a node sends out a Prepare message to a quorum (or all? XXX) of
acceptors with a unique proposal number, higher than any it has sent out before.

\subsubsection*{Phase 1b}

Upon receipt of a Prepare message with proposal number $n$, an Acceptor responds with a Promise
message if it has not responded to any Prepare messages with a number greater than n. The Promise
message contains the highest proposal number, if any, the Acceptor has accepted, and the value it
has accepted.

\subsubsection*{Phase 2a}

In order for a Proposer to move from Phase 1a to Phase 2a, it must receive Promise messages from a
quorum of acceptors.

\subsubsection*{Phase 2b}

If an Acceptor receives an AcceptRequest message from 

\subsection{MultiPaxos}

MultiPaxos is multiple rounds of Paxos occuring at the same time. The way this is outlined in
\emph{Paxos Made Simple} is by a form of leader election. I will outline this here but, for
simplicity, in my project I will simply use multiple Paxos rounds to form the basis of a
distributed operation log. This will be explained in more detail in the Implementation chapter.

\section{ACID}

\subsubsection*{Atomic}

Atomicity means that either an operation completes, or it does not, ie, that the database is not
left in a "halfway" state. This means that we do not need to worry about cleaning up after an
operation, if it does not succeed we can simply retry it, without needing to worry about the state
it has left the database in. Atomicity also applies to transactions in the same way - if we have
a transaction as an operation composed of smaller single operations (for example, INCR B, READ A
$\rightarrow$ X, WRITE X + 10 $\rightarrow$ A), we don't want some of the operations to complete
and some not to. In this example, we may have the constraint $A = 10\times B$. If B was incremented but then
the transaction failed before A was updated, we would leave the database in an inconsistent state.

\subsubsection*{Consistent}

Consistency is very closely related to atomicity, as consistency refers to a property of the
database, and atomicity refers to a property of operations performed on the database. In the
example used for atomicity, we had a constraint on the database (that $A = 10\times B$). We want operations
(or transactions) to transform the database from one consistent state to another. This becomes
more pertinent in distributed databases, as we may receive operations in one order at one node,
and in a different order at another node. If we apply the operations in the order that we receive
them, the two nodes are likely to be in inconsistent states. XXX: define consistency in a
distributed system.

\subsubsection*{Isolated}

Isolation is the property that ensures that if one transaction is in the process of completing,
another transaction cannot see the intermediate state. It ensures that transactions that execute
concurrently to each other are also invisible to each other - a transaction cannot tell, and does
not need to worry about whether another transaction is executing or not.

\subsubsection*{Durable}



\section{Software Engineering}

\subsection{Module Dependencies}

I chose Twisted in order to make implementing the protocol easier. Twisted provides a lot of
support for implementing protocols and helper classes etc. Also as I am using a state machine
approach to implementing DBP and Paxos, twisted's asynchronous system works very well with this
approach.

\subsection{Programming Language}

There were several options for which programming language to use for this project.  C compiles to
very fast code, and gives a lot of control over the behaviour of the whole system.  However it is
very verbose Another option was erlang. Erlang was designed for networking and exhibits a high
degree of parallelism. However it is a very different paradigm and I have never used it before.
In the end I went with Python, a language I am very familiar with, and which is very easy to
prototype and do rapid development in.

\section{Requirements Analysis}

Requirements for my system:
Transactions
ACID properties

\subsection{Software Development Process}

I considered using the waterfall model, but it didn't fit in with my requirement to do fast
prototyping (as I needed to gain a better understanding of Paxos). I settled on the spiral model
because of XXX:

\subsection{Version Control and Backup Strategy}

For version control I decided to use Git, as it is a system I am familiar with, and serves my need
both as a VCS and as a remote backup. In my experience it is more usable than other DVCSs such as
Bazaar, Darcs or Mercurial, and much faster than centralised VCSs such as CVS or SVN. Using Git I
backed up my project both to bitbucket, an online repository service which provides free private
repository hosting, and to my own server.

For backups I use the PWF to develop my project on. As it is stored in Git I can back it up simply
by pushing the repo to other hosts. I currently have it backed up to two other hosts.

\subsection{Testing}

Testing is really important
network
lots of effects
lots of subsystems

\subsubsection{Unit Testing}

unit testing really key
regression testing
testing different subsystems

\subsubsection{Integration Testing}

test program etc
see simple change propagate across complex system



\cleardoublepage
\chapter{Implementation}

\section{Paxos Design}

\subsection{Protocol Design}

I started off with Paxos. I wanted to iterate quickly from prototype to prototype, adding features
slowly as I understood the protocol more, as I found it confusing and wasn't sure how to implement
in code various concepts outlined in the academic papers I read (mainly Paxos Made Simple).

My initial prototype was a synchronous model that sent messages internally. I quickly decided to
change to Twisted, as I hoped the support it would give me would make writing the protocol easier.

\subsubsection*{Messages}
I initially decided to use a class hierachy to define messages, and to use a simple
serialization/deserialization technique, transmitting messages in the form
\verb+"<message type>":<proposal serialization>+. I decided to send messages as simple strings
over the network for a number of reasons - for a prototype implementation speed was not my primary
concern, iterating towards the most complete and correct solution in a reasonable period of time
was. Furthermore, even if my priority was speed, optimising the message format felt like a
premature optimisation, and using a binary format would vastly hinder my debugging.

In hindsight restricting messages to only a combination of message type and proposal attributes
was unnecessarily restrictive. An advantage of constructing them in this way was to prevent typos
in creating messages (cf. \verb+send(Msg({"msg_type": "accept_requst", ...}))+
and \verb+send(Msg({"msg_type": "accept_request", ...}))+).

Although security was not a major concern for this project, I wanted to be able to serialize
arbirtrary objects without allowing remote code execution on a host running my software - even
though it was only running on local host this still seemed unnecessarily risky. Fortunately Python
has a builtin function called \verb+literal_eval+ which only evaluates literals (strings, tuples,
lists and dictionaries), and nothing unsafe (classes, functions) which could be used to run
arbritrary code.

I eventually moved to sending a dictionary in plain string format over the network. This allowed
me to specify arbitrary key/value pairs without having to add in extra support for them (a
limitation I initially struggled with before moving to this format). This greatly simplified a lot
of logic, at the cost of trusting that messages received are well formed. However, this is not too
problematic for several reasons - a) byzantine b) exceptions / paxos dropping c)
checksum/netstring.

\subsubsection*{Hosts}

I first used a tuple of \verb+(IP Address, Port Num)+ to identify hosts. However there turned out
to be a number of problems with this. Firstly as an identifying scheme it is not persistent across
interfaces. Also there is a significant problem if a node needs to identify itself (a pertinent
example is for the \op{TRYLOCK} operation). After I changed the message format to a generic
dictionary format, I wanted to add an attribute specifying the sender. This is difficult to do
using tuple format, as it is non-trivial for a host to obtain its own IP address, it may be on a
local network or behind a NAT for example.

I updated it to use a GUID. This is better for several reasons - the host knows its own address
etc

There is the problem of a node lying about its identity, for example forging a \op{TRYLOCK}
request. This is a more pertinant problem because I am using UDP, which is easier to forge than
TCP. Although a fully secure implementation is beyond the scope of this project, one potential
solution would be to sign or authenticate messages, and include this signature or MAC with the
message.

\subsection{Protocol Extras}

On top of the basic Paxos protocol I added some extra features to the protocol, in particular,
node discovery and bootstrap; and heartbeat monitoring of nodes to detect them leaving the
network.

Keeping an accurate idea of network membership is a key requirement of Paxos. Each node in the
network needs to have a good idea of the number of nodes in the network in order to have an
accurate estimate of the quorum size. If a node underestimates the quorum size, the network may
become inconsistent, as a node may "learn" a value erroneously. On the other hand, if we
overestimate the quorum size, we may not make any progress, waiting for more responses than there
are nodes in the network. In practice, the first problem is more problematic than the second,
which is only temporary - as long as the node eventually accurately learns the quorum sizethe
network will make progress, however if the network develops inconsistencies these are much harder
to resolve.

Consensus algorithms need a strong failure detector (paper reference?).

\subsubsection{Bootstrap}

A node connects to the network by connecting to a \emph{bootstrap node}. In my current
architecture this can be any node, however in a different model it may be a specific node, see the
Evaluation chapter for more details involving scaling. When a node $N$ connects to the bootstrap
node $B$, it sends an \msg{EHLO} message. $B$ replies with a \msg{Notify} message containing all the
hosts B is aware of. $N$ then sends each of these in turn \msg{EHLO} message, making each of them
aware of its presence, and getting a \msg{Notify} message from each othem. This is repeated until
there are no nodes it has not learned of. $N$ is then fully integrated into the network.

\subsubsection{Heartbeat}

In order to identify when a node leaves the network, when a node initialises it starts a timer on
a configurable timeout and sends a \msg{PING} message to every node its aware of. As it receives a
reply from a host it removes that host from the ... XXX.

\subsection{MultiPaxos}

The simple way MultiPaxos is implemented is multiple instances of Paxos operating in parallel.
Paxos is implemented as a state machine, with the instance state as a Python dictionary and
transitions as methods. A simple way to implement MultiPaxos is to have every transition method
take an instance dictionary as an argument and operate on that. One problem with this approach is
that if any initial messages (\msg{Prepare}, \msg{AcceptRequest}, etc) are not received, the
initial state is not constructed.

For certain messages that are not linked to any particular instance of Paxos, the message
attribute \verb+"instance_id"+ is sent with value \verb+None+, for all other messages the instance
id is a number referring to the instance of Paxos running.

\section{Database Design}

\subsection{Design}

\subsubsection{Basic Operations}

The database is implemented as a datastructure that can have operations performed on it. These
operations can be serialised and deserialised. The main problem for designing a distributed
database then becomes deciding on an order for these operations that is consistent across every
node.

\subsubsection{The Operation Log}

Deciding on a serialisation for operations is done by the operation log. This associates an
"Operation ID" (OID) with a particular operation. In order to decide on an OID for an operation,
the code picks the next highest instance ID it hasn't seen before and starts a new round of Paxos
for that instance, trying to assert that \verb$OID := <op>$. I tried a number of different
techniques for retrying in the event that the OID was associated with another operation. This is
always safe because of Paxos. In fact, this technique can even be used to learn all the database
operations performed so far, albeit reasonably inefficiently, simply by performing a READ.

XXX: check retry semantic divide between paxos/db


\subsubsection{Reads}

For a distributed database there is the concept of "fast reads" and "slow reads". A fast read is
simply examining the state of the local copy of the database. This is fast, but may return out of
date data. In order to actually perform an examination of the current state of the database, we
must insert a READ operation, to ensure that if we have an out of date copy of the database, we
obtain the more recent transactions before returning the result to the user.

In fact, I chose to use a NOP operation rather than a READ operation, although they would
semantically be the same (as other nodes do nothing on a READ), as I felt a NOP better represented
the operation being sent (do nothing).

\subsubsection{Transactions}

My first implementation of transactions was a very na\"ive global lock. In order to start a
transaction, a node inserts the TRYLOCK operation. If, when the operation is inserted, the number
of TRYLOCKs is well-bracketed (ie, there is an equal number of lock takes and releases), then the
node was successful in taking the lock. While a node holds the lock, no other node can perform an
operation (the operations are inserted in the transaction log but ignored).

\section{Paxos Implementation}

\subsection{State Machine Architecture}

In Paxos Made Simple, the actions of a node are split into three main roles - {\bf Proposer},
{\bf Acceptor}, {\bf Learner}.

Mixins
getattr

\begin{tabular}{ | c | c | p{7cm} | }
  \hline
  {\bf Message type} & {\bf Handler Class} & {\bf Message action} \\ \hline
  AcceptRequest & Acceptor & Respond with AcceptNotify (if valid) and accept Proposal. \\ \hline
  AcceptNotify & Learner & Record response and if a quorum has accepted that proposal, learn it. \\ \hline
  EHLO & Node & Respond with notify. \\ \hline
  Notify & Node & Add hosts to host list. \\ \hline
  Ping & Node & Send pong.  \\ \hline
  Pong & Node & Remove host from timeout list. \\ \hline
  Prepare & Acceptor & Respond with Promise (if valid). \\ \hline
  Promise & Proposer & Respond with AcceptRequest and deal with timeouts. \\ \hline
\end{tabular}



\section{Database Implementation}

SQL
How tools affected things
Optimisation - "what it is"
Talk about iterations

SQL parser
- what it supports

-start up costs
  - ping time etc
  - inefficiencies
  - cf. "supernodes" vs DHTs to organise nodes

Talk about laptop breaking

\section{Testing}

I started off implementing unit tests for the 

\section{Commandline Tools}

500 words


\cleardoublepage
\chapter{Evaluation}

This is where the second most amount of marks are gained.

Talk about scaling - different types of hierachy - DHT, supernodes etc

\section{Testing}

\begin{enumerate}
	\item unit test inertia
	\item test programs, see complex effects of single change.
	\item durable - network - stable storage
\end{enumerate}



\cleardoublepage
\chapter{Conclusion}

Conclude here.




\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the bibliography

\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{refs}
\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the appendices
\appendix

\chapter{Project Proposal}

\input{propbody}

\end{document}
